[{"title":"Bean作用域","url":"/2022/03/17/Bean%E4%BD%9C%E7%94%A8%E5%9F%9F/","content":"Bean的作用域\nSingleton\n\nBean的作用域默认是单例的\n\nPrototype\n\n每次创建都会返回一个新的Bean实例\n\nrequest 和Session 在web中有效\n\nrequest 每次的HTTP请求都会产生一个新的Bean  仅在当前http请求中有效\nSession 每一次来自新 session的HTTP请求都会产生一个新的 bean该bean仅在当前HTTPsession 内有效。\n怎么配置Spring bean 的作用域\n\n在XML中配置\n\n&lt;bean id=&quot;......&quot;  class=&quot;......&quot;   scope=&quot;Singleton&quot;&gt;&lt;/bean&gt;\n\n\n使用注解@Scope 配置\n\n@Repository@Scope(&quot;prototype&quot;)public class UserDao &#123;    public String hello() &#123;        return &quot;userdao&quot;;    &#125;&#125;\n\n\n\n","categories":["框架"],"tags":["Spring"]},{"title":"Go语言学习","url":"/2022/04/16/Go%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/","content":"Go语言学习笔记2022年04月13日第一个hello,world程序.\npackage mainimport &quot;fmt&quot;func main() &#123;\tfmt.Println(&quot;hello,world&quot;)&#125;\n\n一个go文件只属于一个包,go文件的开头必须是 main包下 go 分为两种 ,一种是可执行的包,一种是工具包.工具包不能执行,但是拓展了主应用的功能\ngo语言命名规则需要对外暴露的变量的名字首字母必须大写,不需要对外暴露的变量名首字母用小写\n包名尽量用小写,尽量不要和标准库的命名冲突,不要用大小写混合或下划线命名\n2022年04月14日简短定义变量 变量名:&#x3D;值 这种方式的缺点是\n\n无法定义数据类型\n只能在函数内部\n定义变量同时显示初始化\n\nvar hp int hp:&#x3D;10\n\n报错信息no new variables on left side of :&#x3D;\n\n这样的话程序会报错 因为在第一行hp已经被声明过了\nconn, err :&#x3D; net.Dial(“tcp”, “127.0.0.1:8080”)conn1, err :&#x3D; net.Dial(“tcp”, “127.0.0.1:8080”)在多个简短变量声明和赋值中,至少有一个新的变量出现在左值中\nSwap a and bpackage mainimport &quot;fmt&quot;func main() &#123;\ta := 1\tb := 2\tvar t int\tt = a\ta = b\tb = t\tfmt.Println(a)\tfmt.Println(b)\t//第二种方法\tc := 1\td := 2\tc, d = d, c\tfmt.Println(c)\tfmt.Println(d)&#125;\n\n\n2022年04月15日匿名变量匿名变量不占用空间,也不会分配内存,多个匿名变量之间没有任何影响定义一个函数getdata()\npackage mainimport &quot;fmt&quot;func getdata() (int, int) &#123;\treturn 10, 100&#125;func main() &#123;\ta,_ :=getdata() //定义第二个接收值为匿名变量,在后续代码中不能被调用\t_,b :=getdata() //定义第一个接收值为匿名变量\tfmt.Println(a)\tfmt.Println(b)&#125;\n\n变量的作用域局部变量 是在函数内声明的变量,局部变量只在函数内有效,函数调用结束后,变量就会被销毁.\n全局变量是在函数外部声明的Go语言函数内可以声明和全局变量相同名称的变量,在使用时,会优先使用函数内的局部变量.\n形式参数\nfunc test(形式参数)&lt;返回值类型&gt;{    函数体}\npackage mainimport &quot;fmt&quot;/** * @ Author     ：zhaolengquan. * @ Date       ：Created in 23:44 2022/4/15 * @ Description： */func main() &#123;\tvar a = 10\tvar b = 20\tfmt.Printf(&quot;main()函数中a=%d\\n&quot;, a)\tfmt.Printf(&quot;main()函数中b=%d\\n&quot;, b)\tc := sum(a, b)\tfmt.Printf(&quot;sum函数的返回值是%d&quot;, c)&#125;func sum(a, b int) int &#123;\tfmt.Printf(&quot;sum函数中a=%d\\n&quot;, a)\tfmt.Printf(&quot;sum函数中b=%d\\n&quot;, b)\treturn a+b&#125;\n\n2022年04月17日22:43:14强制类型转换\n    var f1 float64 = 2.333f2 := int(f1)fmt.Println(f1, f2)//输出结果 2.333 2\n\nString转int\npackage mainimport (\t&quot;fmt&quot;\t&quot;strconv&quot;)func main() &#123;\tstring1 := &quot;112233&quot;\tatoi, err := strconv.Atoi(string1)\tfmt.Println(string1, atoi, err)&#125;\n\n判断字符串是否以某个字符开头或结尾\npackage mainimport (\t&quot;strings&quot;\t&quot;fmt&quot;)func main() &#123;\tstr := &quot;hexo&quot;\tfmt.Println(strings.HasPrefix(str, &quot;h&quot;)) //判断str是否以h字母开头\tfmt.Println(strings.HasSuffix(str, &quot;o&quot;)) //str是否以o字母结尾\t//输出结果 true true\t//Containsany(s string,char string )s包含char中任意一个字符\tprintln(strings.ContainsAny(&quot;abc&quot;, &quot;ddd&quot;))  //返回false\tprintln(strings.ContainsAny(&quot;abc&quot;, &quot;ddda&quot;)) //返回true\t//Contains(s string,substring string) s中包含substring 返回true\tprintln(strings.Contains(&quot;abc&quot;, &quot;ab&quot;)) //返回true\t//Compare方法比较字符串 如果相等返回0\tprintln(strings.Compare(&quot;abc&quot;, &quot;abc&quot;))\t//判断substr在s中的位置\tprintln(strings.Index(&quot;abcde&quot;, &quot;cd&quot;))\t//转换成大写字符\tstrings.ToUpper(&quot;abc&quot;)&#125;\n\n\n\n\n\n\n\n","categories":["Go语言"]},{"title":"JVM笔记","url":"/2022/09/03/JVM%E7%AC%94%E8%AE%B0/","content":"\nJava中99%的对象放在堆中，堆是垃圾回收主要操作区域。\n新生代分为eden区，survivor1和survivor2区 比例是8：1：1\n新生代的对象在经过15次GC之后进入老年代，大对象直接进入老年代。\n绝大多数最新被创建的对象会分配到新生代，大部分对象在创建之后很快变得不可达，所以很多对象被创建在新生代，然后消失。对象从这个区域消失的过程我们称之为 minor GC。\n老年代\n对象没有变得不可达，并且从新生代中存活下来，会被拷贝到这里。其所占用的空间要比新生代多。也正由于其相对较大的空间，发生在老年代上的GC要比新生代要少得多。对象从老年代中消失的过程，可以称之为major GC（或者full GC）。\n永久代\n垃圾回收机制JVM垃圾回收机制\nhttps://www.bilibili.com/video/BV1pZ4y197KK?p=9&amp;spm_id_from=pageDriver\nJVM内存中线程共享的是堆和本地方法区,\n线程私有的是栈和程序计数器  栈又分为本地方法栈和虚拟机栈\nGC\nminorGC\nminorgc 发生在年轻代\nmajorgc发生在老年代\nJVM年轻代分为三个区域\n\nEden\nSurvivor(from)\nSurvivor(to)\n\nEden和两个Survivor区 默认比例 8:1:1\n为什么要分Eden和Survivor区,\n因为新生代发生的gc比较频繁,如果不分Survivor区,那么Eden发生gc后 存活的对象直接被送到老年代,老年代会很快被填满. 而发生在老年代的Fgc消耗的时间要比ygc长的多.\n设置两个Survivor区最大的好处就是减少了内存的碎片化\n\n年轻代里的minorgc发生的特别频繁\n新产生的对象大部分都放在Eden区\n新建的对象都放在Eden区中,\n第一次ygc\nEden区的大部分都会被回收 -&gt;s0\n第二次ygc\nEden区的和s0活着的对象-&gt;s1\n第三次ygc\nEden区和s1活着的对象-&gt;s0\n…\n…\n每次换区(s0-&gt;s1&#x2F;s1-&gt;s0)\n对象的年龄都会+1\n多次垃圾回收后, 当对象的年龄到了,就会被放到老年代(Old区)\n再后来, 老年代满了之后 会出发FullGC\n对象分配规则\n大对象直接分配到老年代,避免Eden区和Survivor区大量的内存拷贝\n(动态年龄判断)\n当Survivor区相同年龄的所有对象加起来大于Survivor区的一半 那么大于等于这个年龄的对象可以进入老年代\n\n垃圾回收器\n针对年轻代\n\nSerial 最基本的,发展历史最悠久的收集器工作环境是单线程, 收集器进行垃圾回收的时候,必须暂停其他所有的工作线程 (Stop The World)几十M &#x2F;几M内存用Serial完全没问题如果内存很大, STW时间会特别长\nParallel Scavenge并行清理垃圾(多线程) 并不是线程数越高效率越高  ,线程的上下文切换比较消耗资源并行收集::指多个垃圾回收器同时收集垃圾,此时业务逻辑线程暂停工作\nParNew收集器 Serial收集器的多线程版本, 需要STW,使用的是复制算法\nCMS (concurrent mark sweep)并发垃圾回收用的是标记清除算法,会产生内存碎片这款收集器第一次实现了让垃圾收集线程与用户线程同时工作.CMS的关注点是尽可能缩短垃圾回收时,用户线程的停顿时间.停顿时间越短,越适合与用户交互的系统.JDK1.8以上 CMS建议升级为G1\nSerial Old收集器是Serial收集器的老年代版本, 主要针对老年代,\nParallel Old收集器： 是Parallel Scavenge收集器的老年代版本，使用多线程，标记-整理算法。\n\n常用垃圾回收算法标记清除算法\n标记到的垃圾可以直接回收,缺点就是内存碎片化,浪费空间\n这里所谓的清除并不是真的置空,而是把需要清除的对象地址保存在空闲的地址列表里。下次有新对象需要加载时,判断垃圾的位置空间是否够,如果够,就存放。\n缺点 \n需要扫描两遍,效率较低GC的时候需要停止整个应用程序,用户体验感较差\n这种方式清理出来的空闲内存是不连续的,产生内存碎片。需要维护一个空闲列表\n拷贝算法(用在年轻代)\n把一片内存分成大小相等的两块,每次只使用其中的一块,\n当这一块的内存用完了,就把存活的对象复制到另外一块上面,然后把已使用的内存一次清理掉\n优点是 保证了内存的连续可用\n缺点是比较浪费空间(内存只能使用一半) 代价较高.\n标记压缩\n比标记清除多了一步整理,解决碎片化问题\n整理压缩阶段,不是对标记的对象回收, 而是将存活的对象都向一端移动,然后擦除边界以外的内存\n可以看到，标记的存活对象将会被整理，按照内存地址一次排列，而被标记的内存会被清理掉。如此一来，当我们需要给新对象分配内存时，JVM只需要持有一个内存的起始地址即可，这比维护一个空闲列表少开销多了\n","categories":["JVM"]},{"title":"MySQL新建索引规则","url":"/2022/03/30/MySQL%E6%96%B0%E5%BB%BA%E7%B4%A2%E5%BC%95%E8%A7%84%E5%88%99/","content":"索引越多越好吗？索引并不是越多越好，\n\n索引会占用较多的磁盘空间\n创建和维护索引需要耗费时间，而且索引过多会影响其他语句的执行效率\n\n索引的创建\n数据量较少的时候不建议创建索引，如果优化器判断出全表扫描比走索引更高效的时候，就不会选择走索引。\n对查询频率较高的字段创建索引，会大幅提高性能\n尽量能覆盖到常用的查询字段\n尽量选择数据量小的字段做索引，如果索引列字段较大，占用空间太大，则会影响查询效率\n\n","categories":["MySQL"],"tags":["索引"]},{"title":"MySQL实战45讲总结","url":"/2022/03/12/Mysql/","content":"MySQL自增主键\n页分裂 页合并\n回表操作\n二级索引放的数据是主键ID\n\n\n2022-03-10 10:27:44\nMySQL缓存 (8.0以后已经弃用), 对基础表的任何修改 都会使所有的缓存失效,\n如果想要一条SQL命中缓存,需要和之前存入缓存的SQL完全一致,参数顺序不同或 多了空格 都会导致不会命中缓存.\n除非表是一张静态表 没有更新只有查询的需求 可以使用缓存提高效率\n如果没有命中缓存 SQL就来到了解析器 首先进入分析器  进行  词法分析 然后语法分析(看语句是否合法)\n分析器任务完成后, 来到优化器\n优化器的作用是 索引选择 和表连接顺序(join)\n一条查询语句的执行需要经过\n连接器-分析器-优化器-执行器\n连接器(建立连接,权限认证)\n分析器(分析词法,语法是否正确)\n优化器(选择索引和连接方式)\n执行器(执行之前需要看用户有没有对表的查询权限)\n","categories":["MySQL"],"tags":["MySQL"]},{"title":"Mysql行锁对性能的影响","url":"/2022/05/06/Mysql%E8%A1%8C%E9%94%81%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D/","content":"怎么减少行锁对性能的影响\n\n在一个事务占有行锁时,另外一个事务必须等到第一个事务释放行锁,才能对该行进行操作.\n事务B会一直阻塞,直到事务A完成后释放行锁,\n在innodb事务中,行锁是在需要的时候才加上的,并不是不需要了就立即释放,而是等到事务结束,也即是commit之后才释放,这就是两阶段锁协议\n怎么减少行锁对性能的影响:\n如果一个事务中需要锁多行, 把最有可能发生锁冲突的,影响并发的锁,尽量往后放,比如实现一个电影院的在线交易业务,业务需要设计以下操作\n\n扣除顾客A的账户余额\n电影院余额加上A购买的票价\n记录日志交易\n\n完成这笔交易需要更新两条记录,为了保证交易的原子性,需要把这三个操作放在一个事务中,此时如果顾客B也要买电影票,那么事务发生冲突的部分就是语句\n2,修改电影院余额.根据两阶段协议,不论怎么安排语句,行锁都是事务提交之后才释放的,所以,如果把语句2放到最后,按照3,1,2的顺序执行,那么影响影院余\n额行锁的时间就最少,最大限度的减少了事务之间的锁等待,提升了并发度.\n减少锁等待时间也并不能完全解决问题, 这里 还要提到死锁检测\n死锁的四个必要条件\n\n互斥条件\n请求和保持\n不可剥夺条件\n循环等待\n\n\n\n\n\n事务A和事务B都在互相等待对方释放自己需要的行锁,进入了死锁状态,\n这里有两种处理策略\n\n超时等待,超时时间可以通过参数进行设置 innodb_lock_wait_timeout\n发起死锁检测,发现死锁后,主动回滚死锁链条中的某个事物,让其他事物可以继续执行,参数innodb_deadlock_detect设置为on表示开启死锁检测功能\n\ninnodb中,超时等待时间默认是50秒,如果对于在线服务来说,是不能承受的,所以一般我们使用主动死锁检测,而且innodb_deadlock_detect的默认值本来就是on,也即是开启状态.主动检测死锁,并且快速发现进行处理,也是有额外负担的\n每当一个事务被锁的时候,就要去看它所依赖的线程有没有被别的锁住,如此循环,最后判断是否出现了循环等待,也就是死锁\n每个新来的线程都要判断是否因为自己的加入,导致了死锁,假如有1000个并发线程要同时更新同一行,死锁检测操作就是100万数量级的,虽然最后检测的结果是没有死锁,但期间要消耗大量的CPU资源,就会出现CPU利用率很高,但是每秒却执行不了几个事务的情况.\n避免过度消耗CPU的情况\n\n如果能确保业务一定不会出现死锁,可以临时把死锁检测关闭\n控制并发度, 使用中间件.对于相同行的更新,在进入引擎之前排队,这样在innodb内部就不会有大量的死锁检测工作了.\n\n","categories":["MySQL"]},{"title":"Nginx负载均衡","url":"/2022/03/16/Nginx%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/","content":"什么是反向代理服务器反向代理应该是Nginx使用最多的功能了，反向代理(Reverse Proxy)方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。\n简单来说就是真实的服务器不能直接被外部网络访问，所以需要一台代理服务器，而代理服务器能被外部网络访问的同时又跟真实服务器在同一个网络环境，当然也可能是同一台服务器，端口不同而已。反向代理通过proxy_pass指令来实现。\nNginx配置#HTTP里的配置        upstream mydata &#123;        #mydata是自己起的名字,可以随便起,下面 location的proxy_pass 有用到        \t\t\t\t# weight是权重          \t\t\t\t# 权重的数值越大，被分配到的几率也更大                server localhost:8181 weight=2;                server localhost:8182 weight=1;        &#125;    server &#123;        listen       80; #服务器监听80端口        server_name  localhost;        location / &#123;            root   html;            index  index.html index.htm;                       # 监听地址, mydata 即用户访问的地址            proxy_pass http://mydata  ;        &#125;\n\n\n\n引入Thymeleaf视图解析器必要的start依赖\n    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;    &lt;/dependency&gt;      &lt;dependency&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;    &lt;/dependency&gt;\n\n\n\nJava代码配置localhost:8181package com.example.controller;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;@Controllerpublic class NginxController &#123;\t@RequestMapping(&quot;/&quot;)\tpublic String test() &#123;\t\treturn &quot;Nginx&quot;;\t&#125;&#125;\n\nNginx.html&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;    &lt;meta charset=&quot;UTF-8&quot;&gt;    &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;    localhost8181: hello Nginx&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\n\napplication.ymlserver:  port: 8181    mvc:    static-path-pattern: /**  web:    resources:      static-locations: classpath:/templates/,classpath:/META-INF/resources/,classpath:/resources/,classpath:/static/,classpath:/public/\n\nlocalhost:8182controller不需要改变,application.yml里port改成8182\nhtml稍做变化\n&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;    &lt;meta charset=&quot;UTF-8&quot;&gt;    &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;    localhost8182: hello Nginx&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\n\n\n\n用Maven把项目打成Jar包后 修改html和端口再次打包\n\njava -jar SpringBoottest-0.0.1-SNAPSHOT.jar \njava -jar SpringBoottest-0.0.1-SNAPSHOT2.jar \n访问Nginx  localhost:80\n刷新能得到不同的效果\n\n\n\n\nNginx负载均衡的几种方式普通轮询算法    upstream mydata &#123;            server localhost:8181             server localhost:8182     &#125;server &#123;    listen       80; #服务器监听80端口    server_name  localhost;    location / &#123;        root   html;        index  index.html index.htm;               # 监听地址, mydata 即用户访问的地址        proxy_pass http://mydata  ;    &#125;\n\n基于比例加权平均upstream mydata &#123;        server localhost:8181 weight=5;        server localhost:8182 weight=2;&#125;\n\n基于IP路由负载上面两种方式都有一个问题 如果我们的请求不是无状态的时候(采用Session保存数据),比如我们把登录信息保存到了Session中,那么跳转到另外一台服务器的时候,就需要重新登陆了,此时可以在upstream 配置中加一行 ip_hash, ip_hash首先把每一个请求 先获得请求的ip地址映射成一个hash值,这样的话每个ip都可以固定访问一台后端服务器,解决了Session跨域的问题\nupstream mydata &#123;        server localhost:8181 weight=5;        server localhost:8182 weight=2;        ip_hash;&#125;\n\n\n\n基于服务器响应时间负载分配根据服务器处理请求的时间来进行负载，处理请求越快，也就是响应时间越短的优先分配。\nupstream mydata &#123;        server localhost:8181 weight=5;        server localhost:8182 weight=2;       \tfair;&#125;\n\n\n\n对不同域名实现负载均衡    upstream AdminConfig &#123;            server localhost:8181             server localhost:8182     &#125;        upstream StudentConfig &#123;            server localhost:8181             server localhost:8182     &#125;server &#123;    listen       80; #服务器监听80端口    server_name  localhost;    location /Admin/ &#123;        root   html;        index  index.html index.htm;       # 监听地址, mydata 即用户访问的地址        proxy_pass http://AdminConfig  ;    &#125;        location /Student/ &#123;        root   html;        index  index.html index.htm;       # 监听地址, mydata 即用户访问的地址        proxy_pass http://StudentConfig  ;    &#125;\n\n\n\n","categories":["Nginx"],"tags":["Nginx","负载均衡"]},{"title":"Redis持久化详解","url":"/2022/04/02/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E8%AF%A6%E8%A7%A3/","content":"RDB持久化Redis是基于内存的数据库,如果退出进程,数据将会消失,所以我们可以把内存中的数据持久化到磁盘上,下次开启Redis进程的时候,读取磁盘中的数据到内存中,就可以继续正常使用Redis了\nRDB持久化是Redis服务器把数据库当前的状态,压缩成一个二进制的RDB文件保存到磁盘.\n数据库状态–&gt;RDB文件    RDB文件–&gt;(恢复)数据库状态\n因为RDB文件是保存在硬盘上的,就算Redis服务器停止或者宕机,计算机停机,只要RDB文件存在,Redis服务器就可以用它来还原数据库的状态.\nRDB文件的创建和载入Redis有两个命令可以生成RDB文件,\n\nSAVE\nBGSAVE\n\n他们的区别是:\nSAVE命令会阻塞Redis服务器,直到RDB文件生成之后才能处理其他命令请求\nBGSAVE命令会在后台fork出一个子进程,然后子进程用于生成RDB文件,Redis服务器仍然可以处理命令请求\nBGSAVE采用轮询等待的方式 等待子进程的信号(生成RDB结束)\nRDB文件的载入没有专门的命令执行, 只要Redis服务器在启动的时候检测到RDB文件的存在,就会自动载入RDB文件.\n\n因为AOF的更新频率比RDB文件高\n如果服务器开启了AOF功能,Redis服务器启动的时候就不会载入RDB文件,优先使用AOF文件还原数据库\n只有在AOF功能处于关闭状态时,才会默认载入RDB文件\n\n在BGSAVE命令的执行过程中,处理SAVE,BGSAVE,BGREWRITEAOF命令的方式也不同\n在Redis服务器执行BGSAVE的过程中,因为SAVE和BGSAVE, BGSAVE和BGSAVE都存在竞争关系,所以Redis服务器会拒绝SAVE和BGSAVE的请求\n其次BGSAVE和BGREWRITEAOF命令不能同时执行 如果当前是BGSAVE状态, BGREWRITEAOF将会等到BGSAVE执行后在执行\n如果当前是BGREWRITEAOF状态,服务器将会拒绝BGSAVE命令\n服务器在载入RDB文件的过程中 会一直处于阻塞状态,知道RDB文件载入完成\n自动间隔性保存Redis允许用户通过配置save选项,让服务器每隔一段时间自动运行BGSAVE命令\nsave配置文件\n\nsave 900 1\nsave 300 10\nsave 60 10000\n\n当只要满足三个条件之一,服务器就会执行BGSAVE命令\n\n900秒内至少1次修改\n300秒内至少10次修改\n60秒内至少10000次修改\n\n服务器会根据save选项设置RedisServer结构的saveparams参数\n\n\n\n\n除了saveparams参数外 服务器还维护了一个dirty计数器和一个lastsave属性\ndirty用于记录上次(成功)执行BGSAVE&#x2F;SAVE命令后数据库的修改次数\nlastsave用于记录上次(成功)执行BGSAVE&#x2F;SAVE命令后的Unix时间戳\n\n\n假设图10-8位当前服务器的状态,在经过了301秒之后(1378271101)服务器会执行一次BGSAVE,因为在300秒内数据库发生了至少10次修改.\n\n\n假设BGSAVE命令5秒后执行完成,现在数据库的状态是 dirty计数器被重新置为0, 当前lastsave时间戳被更新为1378271106(1378271101+5)\nRDB文件结构\n\n\nRedis(5字节)\ndb_version(4字节)\ndata_bases\nEOF(1字节)\ncheck_num(8字节)\n\n\n\n文件头部的Redis部分为5个字节保存着”Redis” 这五个字符,通过这5个字符可以快速的检查当前文件是否是RDB文件\ndb_version长度为4字节,记录的是RDB的默认版本号 例”0006”\ndata_bases包含0个或任意个redis数据库\nEOF为1字节,当读到EOF时标志着RDB文件正文内容的结束\ncheck_num是8字节的无符号整数,保存的是一个校验和,check_num的数值是由REDIS,db_version,data_bases,EOF四个部分计算出来的, 作用是可以校验RDB文件是否损坏或错误.\n","categories":["Redis"],"tags":["Redis"]},{"title":"Zookeeper","url":"/2022/05/09/Zookeeper/","content":"zookeeper是树形的目录服务,每个节点都被称为ZNode,每个节点都会保存自己的数据和节点信息\n\n节点可以拥有子节点,也允许少量数据存储在该节点下\n节点分为四大类\nPERSISTENT持久化节点\nEPHEMERAL 临时节点: -e\nPERSISTENT_SEQUENTIAL 持久化顺序节点: -s\nEPHEMERAL_SEQUENTIAL临时顺序节点: -es\n服务端命令\n.&#x2F;zkServer.sh start 开启\n.&#x2F;zkServer.sh stop 停止\n.&#x2F;zkServer.sh status 查看状态\n.&#x2F;zkServer.sh restart 重启\n客户端命令\n.&#x2F;zkCli.sh\nls &#x2F;    查看根节点下的节点\nls &#x2F;dubbo 查看根节点下dubbo节点下的节点\ncreate默认是持久化的节点\ncreate &#x2F;app1  在根节点下创建app1节点\nget &#x2F;app1 获取数据\nset &#x2F;app1 [数据] 设置节点的数据\ndelete &#x2F;app1 删除节点\ncreate &#x2F;app1&#x2F;p1\ndelete all &#x2F;app1   删除带有子节点的节点\nzookeeper的三种Watcher\nNodeCache 只监听某一个特定的节点\nPathChildrenCache 监控一个Znode的子节点\nTreeCache 监控整个树上的所有节点,\ncurator Java Api操作\nCuratorFramework client;  @BeforeAll  public void test() &#123;    RetryPolicy retryPolicy = new ExponentialBackoffRetry(3000, 10);//    CuratorFramework client = CuratorFrameworkFactory.newClient(&quot;localhost:2181&quot;, retryPolicy);//    client.start();    client = CuratorFrameworkFactory.builder()        .connectString(&quot;localhost:2181&quot;)        .retryPolicy(retryPolicy)        .namespace(&quot;zlq&quot;)        .build();    client.start();  &#125;  @AfterAll  public void close() &#123;    if (client != null) &#123;      client.close();    &#125;  &#125;  @Test  public void createNode() &#123;    try &#123;      //创建临时节点,用withMode,因为创建完之后就关闭会话了所以命令行 ls /zlq 看不到p3节点      //使用sleep睡眠20秒 会话关闭期间就能看到p3节点      client.create().withMode(CreateMode.EPHEMERAL).forPath(&quot;/p3&quot;);      Thread.sleep(20 * 1000);    &#125; catch (Exception e) &#123;      e.printStackTrace();    &#125;  &#125;  @Test  //创建多级目录  public void createmoreNode() &#123;    try &#123;      client.create().creatingParentsIfNeeded().forPath(&quot;/p3/p3.1&quot;);    &#125; catch (Exception e) &#123;      e.printStackTrace();    &#125;  &#125;  @Test  public void getNode() throws Exception &#123;    byte[] data = client.getData().forPath(&quot;/p1&quot;);    System.out.println(new String(data));  &#125;  @Test  public void getchildren() throws Exception &#123;    //查询子节点    List&lt;String&gt; list = client.getChildren().forPath(&quot;/&quot;);    System.out.println(list);  &#125;  @Test  public void getstatus() throws Exception &#123;    Stat stat = new Stat();    byte[] bytes = client.getData().storingStatIn(stat).forPath(&quot;/&quot;);    System.out.println(&quot;-------------------------&quot;);    System.out.println(new String(bytes));    System.out.println(&quot;-------------------------&quot;);  &#125;  @Test  public void setData() throws Exception &#123;    //修改数据    //根据版本修改.withVersion    Stat stat = new Stat();    client.getData().storingStatIn(stat).forPath(&quot;/p1&quot;);    int version = stat.getVersion();    client.setData().withVersion(version).forPath(&quot;/p1&quot;, &quot;xiugai3&quot;.getBytes());  &#125;  @Test  //删除单个节点  public void delNode() throws Exception &#123;    client.delete().forPath(&quot;/p1&quot;);  &#125;  @Test  //递归删除节点 可以删除带有子节点的节点  public void delallNode() throws Exception &#123;    client.delete().deletingChildrenIfNeeded().forPath(&quot;/p3&quot;);  &#125;\n","categories":["zookeeper"]},{"title":"Volatile关键字","url":"/2022/03/14/Volatile%E5%85%B3%E9%94%AE%E5%AD%97/","content":"为什么会出现线程不安全的问题volatile既然是与线程安全有关的问题，那我们先来了解一下计算机在处理数据的过程中为什么会出现线程不安全的问题。\n大家都知道，计算机在执行程序时，每条指令都是在CPU中执行的，而执行指令过程中会涉及到数据的读取和写入。由于程序运行过程中的临时数据是存放在主存（物理内存）当中的，这时就存在一个问题，由于CPU执行速度很快，而从内存读取数据和向内存写入数据的过程跟CPU执行指令的速度比起来要慢的多，因此如果任何时候对数据的操作都要通过和内存的交互来进行，会大大降低指令执行的速度。\n为了处理这个问题，在CPU里面就有了高速缓存(Cache)的概念。当程序在运行过程中，会将运算需要的数据从主存复制一份到CPU的高速缓存当中，那么CPU进行计算时就可以直接从它的高速缓存读取数据和向其中写入数据，当运算结束之后，再将高速缓存中的数据刷新到主存当中。\n我举个简单的例子，比如cpu在执行下面这段代码的时候,\nt=t+1;\n\n\n\n会先从高速缓存中查看是否有t的值，如果有，则直接拿来使用，如果没有，则会从主存中读取，读取之后会复制一份存放在高速缓存中方便下次使用。之后cpu进行对t加1操作，然后把数据写入高速缓存，最后会把高速缓存中的数据刷新到主存中。\n这一过程在单线程中运行是没问题的, 在多线程中运行就会出现问题,在多核CPU中,每个线程可能运行在不同的CPU中,每个CPU都有自己的高速缓存,这时就会出现同一个变量在两个高速缓存中的值不一致的问题了.\n假设此时t的值为0,两个线程同时读取了t的值,并存到了各自的高速缓存中,线程1对t进行了加1的操作,此时t的值为1,并且把t的值写回主内存,,但是线程2中高速缓存的值还是0,进行加1操作后,t的值还是1 然后把t写回主内存.此时两个线程都对t的值进行了+1操作,但是主内存中的值不是2 这就出现了数据丢失的问题,线程不安全.\njava中线程安全问题java语言在处理线程安全问题的时候,会有自己的处理机制,比如synchronized关键字, volatile关键字\njava内存模型规定所有的变量都是存在主内存中,每个线程又都有自己的工作内存(高速缓存),线程对数据的所有操作必须在自己的工作内存中进行,而不是直接在主存中操作.并且每个线程不能访问其他线程的工作内存.java中的每个线程都有自己的工作空间,因此多个线程在处理一个共享变量的时候,就会出现安全问题.\n\n共享变量\n\n上面举例的t就是一个共享变量,共享变量就是能够被多个线程访问到的变量.在java中共享变量包括实例变量,静态变量,数组元素, 他们都被存放在堆内存中\nvolatile关键字可见性可见性的意思就是在多线程环境下,一个变量如果被一个 线程修改,其他线程能够立即知道这个线程被修改了,当线程读取这个变量的时候,去主内存中读取而不是自己的工作内存.\n上面的例子中线程2读取变量时从主内存中读取,就能读到最新的线程1修改后的值,然后再进行+1操作,这样两边的数据就一致了.\n如果一个变量被声明为volatile,那么这个变量就具有了可见性的特征,这就是volatile的作用之一\n缓存一致性协议线程中的处理器会一直在总线上嗅探其内部缓存中的内存地址在\n其他处理器的操作情况，一旦嗅探到某处处理器打算修改其内存地址中的值，而该内存地址刚好也在自己的内部缓存中，那么处理器就会强制让自己对该缓存地址的无效。所以当该处理器要访问该数据的时候，由于发现自己缓存的数据无效了，就会去主存中访问。\n有序性实际上,当我们把代码写好之后,虚拟机不一定会按照我们写的代码的顺序来执行,例如下面两行代码\nint a = 1;int b = 2;\n\n对于这两句代码，你会发现无论是先执行a &#x3D; 1还是执行b &#x3D; 2，都不会对a,b最终的值造成影响。所以虚拟机在编译的时候，是有可能把他们进行重排序的。\n为什么要重排序呢,\n假如执行int a&#x3D;1 需要100ms的时间,执行int b&#x3D;2需要1ms的时间,并且这两行代码并没有依赖关系,最终不会对ab的值造成影响,那肯定先执行b&#x3D;2这行代码了.\nvolatile真的能保证一个变量的线程安全吗我们通过上面的讲解，发现volatile关键字还是挺有用的，不但能够保证变量的可见性，还能保证代码的有序性。那么，它真的能够保证一个变量在多线程环境下都能被正确的使用吗？答案是否定的。原因是因为Java里面的运算并非是原子操作。\n原子操作原子操作：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。也就是说，处理器要嘛把这组操作全部执行完，中间不允许被其他操作所打断，要嘛这组操作不要执行。刚才说Java里面的运行并非是原子操作。我举个例子，例如这句代码\nint a=b+1;\n\n处理器在处理代码的时候，需要处理以下三个操作：\n\n从内存中读取b的值。\n进行a &#x3D; b + 1这个运算\n把a的值写回到内存中\n\n而这三个操作处理器是不一定就会连续执行的，有可能执行了第一个操作之后，处理器就跑去执行别的操作的。\nvolatile无法保证原子性的例子public class test4 &#123;\tprivate static volatile int i = 0;\tpublic static void main(String[] args) &#123;\t\tfor (int j = 0; j &lt; 10; j++) &#123;\t\t\tnew Thread(() -&gt; &#123;\t\t\t\tfor (int k = 0; k &lt; 1000; k++) &#123;\t\t\t\t\ti = i + 1;\t\t\t\t&#125;\t\t\t&#125;).start();\t\t&#125;\t\tSystem.out.println(i);\t&#125;&#125;\n\n\n例如：线程1读取了t的值，假如t &#x3D; 0。之后线程2读取了t的值，此时t &#x3D; 0。然后线程1执行了加1的操作，此时t &#x3D; 1。但是这个时候，处理器还没有把t &#x3D; 1的值写回主存中。这个时候处理器跑去执行线程2，注意，刚才线程2已经读取了t的值，所以这个时候并不会再去读取t的值了，所以此时t的值还是0，然后线程2执行了对t的加1操作，此时t &#x3D;1 。这个时候，就出现了线程安全问题了，两个线程都对t执行了加1操作，但t的值却是1。所以说，volatile关键字并不一定能够保证变量的安全性。\n什么情况下volatile能够保证线程安全刚才虽然说，volatile关键字不一定能够保证线程安全的问题，其实，在大多数情况下volatile还是可以保证变量的线程安全问题的。所以，在满足以下两个条件的情况下，volatile就能保证变量的线程安全问题：\n\n运算结果并不依赖变量的当前值，或者能够确保只有单一的线程修改变量的值。\n变量不需要与其他状态变量共同参与不变约束。\n\n","categories":["JUC"],"tags":["volatile"]},{"title":"MySQL索引分析_01","url":"/2022/03/13/index/","content":"\n表结构如下\n\n表名为T,主键是id 自增, 新建索引name.\nCREATE TABLE `T` (  `id` int NOT NULL AUTO_INCREMENT,  `name` varchar(255) DEFAULT NULL,  `age` int DEFAULT NULL,  `adress` varchar(255) DEFAULT NULL,  PRIMARY KEY (`id`),  KEY `test` (`name`)) ENGINE=InnoDB AUTO_INCREMENT=18 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;\n\n\n\nusing IndexEXPLAIN SELECT id from t where name =&#x27;lisi&#x27;;\n\n使用了覆盖索引 要查的id 就在name索引的叶子结点上\nusing where在使用索引的情况下,需要回表查询数据\nusing where &amp; using IndexEXPLAIN SELECT id from t where name LIKE &#x27;li%&#x27;;\n\n使用了索引,所需要的数据在索引列上都能找到, 不需要回表操作\nusing Index conditionEXPLAIN SELECT * from t where name LIKE &#x27;li%&#x27;;\n\n虽然使用了索引,但是还需要回表查询数据\n索引下推在MySQL5.6以后引入新特性 索引下推\nCREATE INDEX test2 ON T (name,age)EXPLAIN SELECT * from t WHERE name like &#x27;li%&#x27; AND age =10 \n\n在MySQL5.6版本以前 上述SQL执行流程:\n因为name和age是联合索引,但是因为name是范围查询(&gt; &lt; between like) 根据name排序之后age索引就失效了 所以就带着id去回表查询age是否复合条件\n和下图(列名不同)但原理相同\n\nMySQL5.6以后引入索引下推SQL执行流程是\n在根据name查询以后,InnoDB 在 (name,age) 索引内部就判断了 age 是否等于 10，对于不等于 10 的记录，直接判断并跳过。在我们的这个例子中，只需要对 ID4、ID5 这两条记录回表取数据判断，就只需要回表 2 次。\n","categories":["MySQL"],"tags":["索引"]},{"title":"innodb读书笔记","url":"/2022/09/03/innodb%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/","content":"读书遇到的问题\n有没有一种可能是某一页同时存在flush链表和LRU链表呢？\np58刷新邻接页\n为什么iops比较高的固态硬盘可以关闭刷新邻接页这个参数呢？而比较老式的机械硬盘建议启用该特性\np72\n逻辑读是91584次，物理读是19次，从此比例可以看出SQL查询还能优化？物理读在全部的读取次数里面占比已经很低了，为什么还能优化呢?\np77\n事务在没commit之前，如果是同步写binlog日志，那么数据库宕机回滚的时候，binlog已经记录了事务信息，为什么不能回滚？\nbinlog记录了信息的事务也不能回滚吗？\nbinlog-cache-size是基于会话的，那如果syc-binlog设置为1，也即是同步写，这样的话binlog-cache-size还会用到吗？是不是事务开启的时候直接写入binlog磁盘了？\np88\nbinlog和redolog的区别\n\n 书上说的是二进制文件只在事务提交前进行提交。即只写磁盘一次，77页有一个参数sync-binlog如果是1的话，是同步写磁盘，为什么？\ninnodb缓冲池（buffer pool）innodb缓冲池不仅缓冲索引，还缓冲行记录，自适应哈希索引，插入缓冲，锁，以及其他数据结构。\n很大的缓冲池面临的挑战：预热和关闭会话费更多的时间，如果很多脏页在缓冲池里，innodb关闭之前需要把脏页写会磁盘文件。也可以强制关闭，但重启的时候必须做更多的恢复工作。\n逻辑存储结构表空间，段，区，页，行\n一个segment段可以申请4~5个区\n一个区里面又有连续的64页，每个区的大小都是1M，一页的大小是16kb。\ninnodb 1.2.x版本之后支持修改页大小，可以修改为8kb、4kb、16kb，那对应的一个区就有128页，256页，32页。修改页大小的参数：innodb_page_size\n新建表的默认大小是 96kb，用的是32个碎片页，碎片页用完之后，才会申请64个连续的页使用。\ninnodb一页最多存放的行数是16kb&#x2F;2-200&#x3D;7992行。\ninnodb默认行格式compact\n数据库实例的作用之一就是读取页中存放的行记录\nMySQL行格式compact行记录格式\n\n\n\n变长字段长度列表\nNull标志位\n记录头信息\n列1数据\n列2数据\n\n\n\n\n\n\n\n\n\n\nNull标志位记录的是可以为null列的空值情况。\n例如有四列，c1,c2,c3,c4\nc2不能为null，c2不参与null标志位\n有一条记录是eeee,fff,null,null\nc2不参与，那么就是c3,c4为null，又要为倒叙排\n\n\n\nc4\nc3\nc1\n\n\n\n1\n1\n0\n\n\n不足一个字节前面补0，\n00000110，16进制表示是0x06\n行记录格式如下\n\ninnodb中null值只在null值列表里存放。\n主键生成策略\n如果表没有主键，会把唯一索引列作为主键，如果还没有的话就使用表默认添加的rowid隐藏列作为主键，\nrowid是在表没有主键和唯一索引列才会定义的。\nchar列在未能占满其定义长度的时候会用空格填充，空格的ascii码表示为0x20\n\nMySQL Varchar类型的最大长度是65535个字节，在latin1编码下，最大支持创建varchar65532大小，并且会出现warning警告，创建完表之后，再查看表信息会发现字段被转换为了TEXT类型\n在GBK编码下最大支持的大小是65532&#x2F;2&#x3D;32767\nUTF-8编码下最大大小是65532&#x2F;3&#x3D;21845\n可以理解varchar（N）N指的是字节长度。\nvarchar需要使用1或2个额外字节记录字符串的长度：如果列的最大长度小于或等于255字节，则只使用1个字节表示，否则使用2个字节。假设采用latin1字符集，一个varchar(10)的列需要11个字节的存储空间。varchar(1000)的列则需要1002个字节，因为需要2个字节存储长度信息。\n此外MySQL官方手册定义的65535长度也是指所有的varchar类型和不能超过65535\n例如下图创建varchar类型总和为66000，也会报错的\ncreate table test2&#123;a varchar(22000),b varchar(22000),c varchar(22000),&#125; charset=latin1 engine=innodb;\n\n\n\n如果页只能存放一条记录，innodb存储引擎会自动将行数据存放到溢出页中。但是如果可以在一个页中至少放入两行数据，那varchar类型的行数据就不会存放到blob页中，这个阈值的长度是8098，超过阈值，数据页保存768字节，其余的保存在blob页中，768字节是前缀数据，后面是偏移量，指向行溢出页。\n对于text和blob类型，也不一定是插入数据页还是blob页，要看具体情况，\n若建一个blob字段的表，插入四行长度为8000的记录，其实也是存放在数据页中。既然用户使用了blob列类型，一般不会存放这么小的数据，大多数的情况下blob的行数据还是会溢出，实际数据保存在blob页中，数据页存放数据的前768字节。\nmysql5.0之后的默认行格式为Compact ， 5.7之后的默认行格式为dynamic\ncompressed和dynamic\n新的行记录格式对于存放在blob中的数据采取了完全的行溢出格式，数据页中只存放20个字节的指针，实际数据全部存放在offpage中。\ncompressed的另一个功能是，存储在其中的行数据会以zlib算法进行压缩，对于blob，text，varchar 这类大长度类型的数据能够进行非常有效的存储。\n在UTF-8 CHAR（10）类型的列，最小可以存储10个字节的字符，最大可以存储30字节的字符。 因此对于多字节字符编码的char数据类型的存储，innodb存储引擎在内部将其视为变长字符类型。也意味着变长长度字段列表会记录char数据类型的长度。\n在多字节字符集情况下，char和varchar实际行存储基本是没区别的。\ninnodb数据页结构\nfile header（记录数据页的头信息）\npage header（记录数据页的状态信息）\ninfimum和supremum Record\nUser-record\nFree space\npage directory\nfile trailer\n\n file header中的属性\n\n表空间中每个页\nfile header 中的file-type引擎页类型，0x45BF是索引页，也即数据页\nfile trailer只有一个fil-page-end-lsn部分，占用8字节，前4字节是该页的checksum校验和的值，后四字节和fileHeader中的fil-page-lsn相同。\n保证数据页从内存刷新到磁盘时数据一致性\n行记录在innodb中的存储格式。\n\nheap_no表示当前记录在记录堆的位置信息，heap-no从2开始的原因是还有两条虚拟记录保存在页里面，他们是infimum和supremum。\n他们不保存在User record部分，而是单独在Infimum + Supremum的部分\nrecord-type 记录类型，0表示普通记录，1表示B+树非叶子结点记录（目录项，存放页最小主键值和页编号），2最小记录，3最大记录\n\nnext-record很重要，表示的是当前数据的真实记录到下一条数据真实记录的地址偏移量。\n比方说第一条记录的next_record值为32，意味着从第一条记录的真实数据的地址处向后找32个字节便是下一条记录的真实数据。\n还需要注意的一点是，下一条记录并不是插入顺序的下一条记录，而是按照主键大小顺序的下一条记录，并且规定**infimum*的下条记录就是本页主键值最小的记录，而本页主键值最大的记录的下记录一条就是Supremum***记录\n最大记录的next-record值为0，这些记录按照主键值从小到大的顺序，形成了一个单链表。\n​                                       \n如果删除一条记录路，会发生下面的变化，\n\n记录并没有从页中删除，而是把delete-mask值设置为1\nnext-record为0，表示没有下一条记录\n如果删除的是第二条记录，那么第一条记录的next-record就会指向第三条记录\n最大记录的n-owned值从5变为4（暂时不懂)\n\n如果删除了一条记录之后再重新插入这条记录，会复用之前的位置。\npage directory槽\n\n在一个数据页中查找指定主键值记录的步骤\n\n通过二分法确定对应的槽，并找到该槽所在分组主键最小的那条记录。\n通过记录的next-record属性遍历槽所在组的所有记录\n\n查找一条记录的过程如果是根据索引查找，就比较方便，因为slot就是根据主键索引建立的，直接在页目录中使用二分法，定位槽，然后遍历槽就能得到记录。\n如果没有索引，只能从最小记录开始遍历查找，这种效率是非常低的。\n存放记录的页在内存中并不是相邻的，但他们都要保证下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。所以插入数据的时候伴随着一个页分裂的过程。\n\n假设页中只能存放三条记录，这些记录按照主键值的大小串联成一个单向链表，前后是虚拟页，最大和最小。\n这时再插入一条主键值为4的记录， 因为页中只能存放三条记录，所以不得不再申请一个新的页面：\n\n新分配的数据页编号可能不是连续的，也就是说我们使用的这些页在存储空间里可能并不挨着，只是维护着一个上一页下一页的编号而建立的链表关系。此外，页10中最大的主键值是5，我们插入的主键值是4，这样就不符合下一个数据页的主键值大于上一个页中用户记录的主键值，所以插入主键值为4的记录伴随着一次记录移动，也就是把主键值为5的记录移动到28页中，然后把主键值为4的记录移动到页10中。\n\n这个过程表明了我们在对页中的记录进行增删查改操作的时候，必须通过一些比如记录移动的操作来保证这个状态一直成立：下一个数据页的主键值大于上一个页中用户记录的主键值。这个过程可以称之为页分裂。\n我们把每个页中用户记录最小的主键值，都筛选出来，用key表示，保存在自己定义的目录项中。\n\n这样就可以实现根据主键值快速查找某条记录。这个目录有一个别名，就是索引。\n上面只是索引的简易方案，是我们为了根据主键值进行二分查找时，使用二分快速定位到具体的目录项，而假设所有的目录都可以在物理存储器上连续存储。\n目录项记录和普通记录的不同点\n\n目录项记录 record-type是1，普通记录的是0\n目录项记录只有主键值和页编号两个列，普通记录的用户记录的列是用户自己定义的，还包含innodb的隐藏列，\n记录头信息中的min-rec-mask，只有在存储目录项信息中的主键值最小的目录项记录的min-rec-mask值为1，其他别的记录的min-rec-mask值是0.\n\n假设目录项记录也被占满了（虽然目录项记录需要的存储空间比用户记录小多了，但是一页就只有16kb，能存放的目录项记录也是有限的）就会新开数据页存放目录项记录。\n\n因为存储的目录项记录不止一个，所以想要根据主键值查找一条用户记录大致需要3个步骤，以查找主键值为20的记录为例：\n\n确定目录项页我门现在的目录项页只有两个，页30和页32，因为页30表示的目录项主键范围是[1，320），页32表示的目录项的主键值不小于320，所以主键值为20的记录在页30中。\n通过目录项记录确定用户记录真实所在页。\n在真实存储用户记录的页定位到具体记录。\n\n在一个存储用户记录的页中通过主键值定位一条用户记录的方式\n当数据非常多的时候，会产生很多的目录项记录的页，怎么快速定位一个存储目录项记录的页呢？ 方法就是再套一层，生成一个更高级的目录，就像多级目录一样。\n\n随着表记录的增加，目录的层级会继续增加，这就是B+树！\n\n从图中可看出我们的用户记录都存放在B+树的最底层的节点上，这些节点被称为叶子节点上，其余的用来存放目录项的节点，称为非叶子节点。B+树最上面的那个节点称为根节点。\n聚簇索引和非聚簇索引聚簇索引只能在搜索条件是主键值时才能发挥作用。因为B+树中的数据都是按照主键进行排序的。\n非聚簇索引也被称为二级索引，普通索引，可以多建几颗B+树，不同的B+树中的数据采用不同的排序规则，如果使用c2列大小进行查找，B+树就要根据c2列进行排序，叶子结点放的记录只有c2列+主键值，目录项存放的是c2列+页号。\n这样我们用普通索引在B+树上查到主键之后还要进行一次回表操作，去主键索引的B+树上查对应的用户记录。\n为什么非聚簇索引的B+树，叶子结点不是直接放用户数据呢？\n因为占用空间太大了。相当于每建立一颗B+树，都要把所有的用户数据再拷贝到新的B+树上，太浪费存储空间。\n联合索引比如我们想让B+树按照c2和c3列的大小进行排序，需要先把各个记录和页按照c2列进行排序，在记录的c2列相同的情况下，采用c3进行排序。\n同时我们需要注意几点，\n每条目录项记录都由c2，c3，页号这三部分组成，各条记录先按照c2列的值进行排序，如果记录的c2列相同，则按照c3列的值进行排序，\nB+树叶子节点处的用户记录由c2、c3和主键c1列组成。\n索引的代价空间代价：\n每建一个索引，就要新建一颗B+树，每颗B+树的每个节点都是一个数据页，一个页默认占用是16kb的存储空间\n时间代价\n每次对表中的数据进行增删查改操作，都需要修改各个B+树索引，数据页之间是双向链表，同一页面中的数据都是按照索引值从小到大的顺序形成的单向链表，增删改会对节点和记录的排序造成破坏，存储引擎需要额外的时间进行记录移位，页面分裂，页面回收等操作来维护好节点和记录的排序。\nwhere条件进行全值匹配的时候，顺序不影响性能，MySQL查询优化器会进行优化，决定搜索条件使用先后顺序。\n联合索引范围查找对于联合索引来说，使用多条件的范围查找，只能使用到第一个索引，比如建立idx_name_birthday_phone_number （name，birthday，phone_number）的联合索引，查询的SQL语句如下\nSELECT * FROM person_info WHERE name &gt; &#x27;Asa&#x27; AND name &lt; &#x27;Barlow&#x27; AND birthday &gt; &#x27;1980-01-01&#x27;;\n\n首先根据name值进行范围查找，查找结果是多条不同的记录，对这些name值不同的记录继续通过birthday的条件进行过滤， 而只有name值相同的情况下，birthday才是排序的，这个查询中通过name进行范围查找的记录，并不是根据birthday列进行排序，所以搜索条件只能用到name索引。\n使用联合索引进行排序使用order by的时候，只能把记录都加载到内存中，再使用排序算法，有时候查询结果集太大，以至于不能在内存中进行排序，还可能暂时借助磁盘的空间存放中间结果，排序完成后再把排好序的结果集返回客户端。如果order by子句里使用到了索引列，就可能省去了在内存或文件中排序的步骤\nSELECT * FROM person_info ORDER BY name, birthday, phone_number LIMIT 10;\n\n因为我们建立的联合索引就是name，birthday，phone_number，所以orderby排序的时候直接去索引取数据，然后进行回表取出索引中不包含的列，非常的方便。\n注意事项：\norderby后列的顺序必须按照索引列的顺序，如果顺序混乱，就用不了B+树索引\n如果是最左匹配原则，还是可以使用部分B+树的索引，当联合索引左边列值为常量，也可以使用后面的列进行排序。\nSELECT * FROM person_info WHERE name = &#x27;A&#x27; ORDER BY birthday, phone_number LIMIT 10;\n\n\n\n回表的代价首先使用二级索引查数据的时候，因为数据是按照索引列值从小到大的顺序排放的。集中分布在一个或几个数据页中，可以很快的把连着的数据从磁盘中读取出来，这种读取方式是顺序io，读出的数据，id字段值可能并不相连，在聚簇索引记录是根据id顺序排列的，回表读取的时候，是随机io\n需要回表的记录越多，使用二级索引的性能越低，甚至有些查询宁愿全表扫描也不使用二级索引。\n查询优化器：事先根据查询条件计算回表记录数，记录数越多，越倾向于全表扫描。反之使用二级索引+回表的方式查询。\n使用覆盖索引避免回表操作\n建议在查询列表里只包含索引列，比如\nSELECT name, birthday, phone_number FROM person_info WHERE name &gt; &#x27;Asa&#x27; AND name &lt; &#x27;Barlow&#x27;\n\n省去了回表操作的性能损耗。\n建立索引考虑因素只为出现在where条件，order by，group by子句中出现的列建立索引。查询条件列没必要建立索引\n考虑列的基数：比方说某个列包含值2, 5, 8, 2, 5, 8, 2, 5, 8，虽然有9条记录，但该列的基数却是3。\n在记录行数一定的情况下，列的基数越大，该列中的值越分散，列的基数越小，该列中的值越集中。\n假设某个列的基数为1，也就是所有的记录在该列中的值都一样，那建立索引是没用的，因为所有的值都一样就无法排序，也就无法快速查找。\n索引列类型尽量小\n\n数据类型越小，查询时进行的比较操作较快（cpu层次）\n数据类型越小，索引占用空间越小，一个数据页就可以放更多的记录，减少磁盘io带来的性能消耗，意味着可以把更多的数据页缓存在内存中，加快读写效率。\n\n这个建议对于表的主键来说更加适用，因为不仅是聚簇索引中会存储主键值，其他所有的二级索引的节点处都会存储一份记录的主键值，如果主键适用更小的数据类型，也就意味着节省更多的存储空间和更高效的I&#x2F;O。\n索引字符串前缀\n使用utf8字符集存储，编码一个字符需要1-3个字节，字符串长的话占用磁盘空间多，这时建立索引，B+树需要把该列整个字符串都要存储起来， 同时做字符串比较的时候占用更多时间。\n比方说我们在建表语句中只对name列的前10个字符进行索引可以这么写：\nCREATE TABLE person_info(  name VARCHAR(100) NOT NULL,  birthday DATE NOT NULL,  phone_number CHAR(11) NOT NULL,  country varchar(100) NOT NULL,  KEY idx_name_birthday_phone_number (name(10), birthday, phone_number));  \n\n二级索引不包含完整的name列信息，所以无法对前10个字符相同，后面字符不同的记录进行排序，所以索引列前缀的方式无法使用索引排序。\n索引列参与计算\nWHERE my_col * 2 &lt; 4\nWHERE my_col &lt; 4&#x2F;2\n\n第一个索引列参与计算，用不到B+树索引\n第二个索引列是单独出现的，可以使用B+树索引。\n为什么引入区的概念如果以页的概念管理B+树，分配存储空间，虽然页之间是以双向链表连接，如果链表中相邻的两个页物理空间非常远的话，相当于是随机io，磁盘速度是非常慢的，所以尽量让链表中相邻页物理位置也相邻，这样进行范围查询的时候可以使用顺序io。\n引入区的概念，一个区就是物理位置上连续的64个页，当表数据量大的时候，分配空间就不是按照页，而是按区为单位分配，甚至表数据非常多的时候，可以一次性分配多个连续的区，虽然可能造成空间浪费，但是从性能角度看，消除了很多随机io，功大于过！\n","categories":["MySQL"]},{"title":"动态代理","url":"/2022/10/06/%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/","content":"编译阶段不用关心代理的类，运行阶段才指定代理哪一个对象。\nInvocationHandler接口是JDK提供的动态代理接口，\n自定义动态代理类实现InvocationHandler接口。\ninvoke方法是必须实现的，它完成对真实方法的调用，\n给定一个接口，动态代理会宣称“已经实现该接口的所有方法了，”动态代理怎么实现被代理接口中的方法呢?\n通过InvocationHandler接口，所有方法都通过Handler处理，即所有被代理的方法都由InvocationHandler接管实际的处理任务。\npublic class GamePlayerIH implements InvocationHandler &#123;    Object object = null;    public GamePlayerIH(Object object) &#123;        this.object = object;    &#125;    @Override    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123;        return method.invoke(object, args);    &#125;&#125;public class Client &#123;    public static void main(String[] args) &#123;        iGamePlayer gamePlayer = new GamePlayer(&quot;张三&quot;);        InvocationHandler handler = new GamePlayerIH(gamePlayer);        iGamePlayer proxyInstance = (iGamePlayer) Proxy.newProxyInstance(gamePlayer.getClass().getClassLoader(),gamePlayer.getClass().getInterfaces(), handler);        proxyInstance.login(&quot;zs&quot;,&quot;123&quot;);        proxyInstance.killBoss();        proxyInstance.update();    &#125;&#125;\n\n\n\n第二种方式\npublic class AutoProxy implements InvocationHandler &#123;    private Singer singer;    public Object getproxy(Singer singer) &#123;        this.singer = singer;        Object instance = Proxy.newProxyInstance(singer.getClass().getClassLoader(), singer.getClass().getInterfaces(), this);        return instance;    &#125;    @Override    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123;        System.out.println(&quot;前置通知&quot;);        Object invoke = method.invoke(singer);        System.out.println(&quot;后置通知&quot;);        return invoke;    &#125;&#125;Client端Star star = (Star) new AutoProxy().getproxy(new Singer());star.show();\n\n第三种方式\npublic class GamePlayerIH &#123;    Object object = null;    public GamePlayerIH(Object object) &#123;        this.object = object;    &#125;    public Object getproxyInstance() &#123;        return Proxy.newProxyInstance(object.getClass().getClassLoader(),object.getClass().getInterfaces(), new InvocationHandler() &#123;            @Override            public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123;                if (method.getName().equalsIgnoreCase(&quot;login&quot;)) &#123;                    System.out.println(&quot;快上号看看!!!有人登录你的账号了！！！&quot;);                &#125;                return method.invoke(object, args);            &#125;        &#125;);    &#125;&#125;Client        iGamePlayer gamePlayer = new GamePlayer(&quot;zs&quot;);    GamePlayerIH gamePlayerIH = new GamePlayerIH(gamePlayer);    iGamePlayer instance = (iGamePlayer)gamePlayerIH.getproxyInstance();    instance.login(&quot;zs&quot;,&quot;123&quot;);    instance.killBoss();    instance.update();\n\n参考博客\n动态代理\n"},{"title":"微服务架构原理深度解析","url":"/2022/10/06/%E5%BE%AE%E6%9C%8D%E5%8A%A1/","content":"注册中心与CAP理论Consistency一致性\nAvailability可用性\nPartition tolerance 分区容错性\n分区容错性：遇到某节点或网络分区出现故障时，仍然能够对外提供满足一致性和可用性的服务\n，例如X和Y节点出现故障，但仍然可以很好的对外提供服务。\nCAP的取舍\n满足CA，舍弃P这就意味着系统不是分布式，因为分布式就是把功能分开部署到不同的服务器\n满足CP舍弃A，意味着系统可能在一段时间内访问失效，不会出现数据不一致的情况\n满足AP舍弃C，意味着系统在并发访问的时候可能会出现数据不一致的情况。\n事实证明，分布式系统，为了避免单点故障，只能从CP和AP中根据业务场景选择一种。\nEureka ：AP，无主备节点之分，一个节点挂了会自动切换到其他节点，实现去中心化，优先保证可用性。\nconsul：CP原则，Consul的Raft协议要求必须过半数的节点都写入成功才能认为注册成功，在Leader挂掉后， 重新选取Leader之前，服务不可用。\nzookeeper：CP，搭建集群的时候，如果某个节点失效，会进行leader选举，或者半数以上节点不可用则无法提供服务。因此可用性无法满足，zk使用paxos算法保证数据一致性\n容错与隔离进程隔离\n进程是操作系统中重要的隔离机制，每个进程都有独立的地址空间，提供操作系统级别的保护区，一个进程的出错不会影响到别的进程，一个应用出错也不会对其他应用产生负作用。\n微服务的最佳载体是容器，容器的本质就是一个进程。\n线程隔离\n进程之间虽然有较好的隔离性，但进程之间交互需要跨进程边界，进程在数据共享方面存在数据传输的开销。\n多线程并发编程模式可以最大限度提高系统并行度，线程是操作系统最小的调度单元，可以更好的利用多处理器的优势。\n线程隔离主要指线程池的隔离，在应用系统内部，将不同请求分类给不同的线程池，当某个服务出现故障，可以根据预先设定的熔断策略阻断线程池的运行。\n\n雪崩效应\n基础服务故障导致级联故障的现象称为雪崩效应。雪崩效应描述的是服务生产者不可用导致消费者不可用，并把不可用逐渐放大的过程。\n例如一个电商平台的下单场景，会经过如下链路 APP客户端-Api网关-账单服务-支付服务-库存服务，但是在错综复杂的网络中各式各样的问题都会造成系统异常，难免有些请求会失败，雪崩效应就此产生。\n服务降级和服务熔断降级和熔断的最终效果都是为了保护系统，防止系统整体崩溃，\n服务降级一般指服务器压力剧增时，根据业务场景使用情况和流量，对一些服务和页面有策略的不处理或用一种简单的方式处理，从而释放服务器资源保证核心业务的正常运行。\n服务熔断是应对雪崩效应的微服务链路保护机制，当调用链路的某个微服务不可用，或者响应时间太长时，会进行服务熔断，不再有该节点微服务的调用，快速返回错误的响应信息。当检测到该节点微服务恢复正常后，恢复调用链路。\n两者之间还是有明确的区别\n\n触发条件区别熔断是自动触发的，降级除了自动触发，还可以根据系统预先设置的代码逻辑进行降级。\n分级的差别熔断通常依赖系统整体框架处理逻辑，每个微服务都需要无差别的具备熔断的特性，降级需要针对业务的优先级和重要性进行分级。对于核心业务，优先级别较高，对于核心系统下游的非核心业务，如果是弱依赖关系，级别可以相对降低。但是如果他们具有强依赖关系，那么非核心业务也会升级为核心业务。\n\n"},{"title":"线程池参数","url":"/2022/04/15/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%AF%A6%E8%A7%A3/","content":"合理利用线程池能够带来三个好处。\n\n降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。\n提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。\n第三：提高线程的可管理性。\n\ncorePoolSize（线程池的基本大小）：当提交一个任务到线程池时，线程池会创建一个线程来执行任务，即使其他空闲的基本线程能够执行新任务也会创建线程，等到需要执行的任务数大于线程池基本大小时就不再创建。如果调用了线程池的prestartAllCoreThreads方法，线程池会提前创建并启动所有基本线程。\n线程池基本工作流程\n\n首先用ThreadpoolExecutor创建一个新的线程池\n\n首先,当任务到达时,运行的线程数&lt;corepoolsize,会新建线程,\n如果运行的线程数&gt;&#x3D;corepoolsize,线程会进入队列,而不添加新的线程\n若队列是new ArrayBlockingQueue&lt;&gt;()有界队列,当队列满了之后,再有新的任务到达,此时还没到达maximumpoolsize,就会新建线程,运行任务\n有界队列ArrayBlockingQueue&lt;Integer&gt; queue = new ArrayBlockingQueue&lt;Integer&gt;(1);\n\n\n无界队列LinkedBlockingQueue&lt;Integer&gt; queue = new LinkedBlockingQueue&lt;&gt;();  \n\n对于无届队列来说,\nnewFixedThreadPool和newSingleThreadExecutor底层用的是无届队列\n\n\n特殊的队列package cn.zlq.SynchronousQueue;import java.util.concurrent.SynchronousQueue;/** * @ Author     ：zhaolengquan. * @ Date       ：Created in 14:34 2022/4/15 * @ Description： */public class Test &#123;\tpublic static void main(String[] args) throws InterruptedException &#123;\t\tSynchronousQueue&lt;Integer&gt; queue = new SynchronousQueue&lt;&gt;();\t\tThread thread1 = new Thread(() -&gt; &#123;\t\t\tSystem.out.println(&quot;put thread start&quot;);\t\t\ttry &#123;\t\t\t\tqueue.put(1);\t\t\t&#125; catch (InterruptedException e) &#123;\t\t\t\te.printStackTrace();\t\t\t&#125;\t\t\tSystem.out.println(&quot;put thread end&quot;);\t\t&#125;);\t\tThread thread2 = new Thread(() -&gt; &#123;\t\t\tSystem.out.println(&quot;take thread start&quot;);\t\t\ttry &#123;\t\t\t\tSystem.out.println(&quot;take from putThread: &quot; + queue.take());\t\t\t&#125; catch (InterruptedException e) &#123;\t\t\t\te.printStackTrace();\t\t\t&#125;\t\t\tSystem.out.println(&quot;take thread end&quot;);\t\t&#125;);\t\tthread1.start();\t\tThread.sleep(1000);\t\tthread2.start();\t&#125;&#125;\n\n先看结果\nput线程执行后就被阻塞了,只有消息被消费后,put线程才可以返回.\n\n默认是非公平 也即是栈结构,公平模式下用的是队列结构\n底层实现请前往博客\nhttps://blog.csdn.net/yanyan19880509/article/details/52562039\n\nnewCachedThreadPool底层用的就是这个队列\n\n","categories":["JUC"],"tags":["线程池"]},{"title":"Zookeeper分布式锁","url":"/2022/05/09/zookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","content":"分布式锁的抢占过程\n客户端A发起一个加锁请求，先会在你要加锁的node下搞一个临时顺序节点，这一大坨长长的名字都是Curator框架自己生成出来的。\n然后，那个最后一个数字是”1”。因为客户端A是第一个发起请求的，所以给他搞出来的顺序节点的序号是”1”。\n接着客户端A创建完一个顺序节点。还没完，他会查一下”my_lock”这个锁节点下的所有子节点，并且这些子节点是按照序号排序的，这个时候他大概会拿到这么一个集合：\n\n接着,客户端A会做一个关键性的判断,看自己创建的临时顺序节点是否排在第一个,如果是的话,就进行加锁操作.\n因为是第一个创建顺序节点的人,所以就是第一个尝试加分布式锁的人.\n\n客户端B前来排队\n客户端A已经加完锁了,客户端B想来加锁,先在my_lock这个锁节点下创建一个临时顺序节点,此时名字会变成类似这个\n\n客户端B因为是第二个来创建顺序节点的，所以zk内部会维护序号为”2”。\n接着客户端B会走加锁判断逻辑，查询”my_lock”锁节点下的所有子节点，按序号顺序排列，此时他看到的类似于：\n\n同时检查自己创建的顺序节点，是不是集合中的第一个？\n明显不是啊，此时第一个是客户端A创建的那个顺序节点，序号为”01”的那个。所以加锁失败！\n客户端B开启监听客户端A\n加锁失败了以后，客户端B就会通过zookeeper的API对他的顺序节点的上一个顺序节点加一个监听器。zookeeper天然就可以实现对某个节点的监听。\n客户端B会对排在他前面的那个节点加一个监听器，监听这个节点是否被删除等变化！大家看下面的图。\n\n接着，客户端A加锁之后，可能处理了一些代码逻辑，然后就会释放锁。那么，释放锁是个什么过程呢？\n其实很简单，就是把自己在zk里创建的那个顺序节点，也就是001节点会被删除\n删除了那个节点之后，zookeeper会负责通知监听这个节点的监听器，也就是客户端B之前加的那个监听器，说：兄弟，你监听的那个节点被删除了，有人释放了锁。\n\n此时客户端B的监听器感知到了上一个顺序节点被删除，也就是排在他之前的某个客户端释放了锁。\n此时，就会通知客户端B重新尝试去获取锁，也就是获取”my_lock”节点下的子节点集合，此时为：\n\n集合里此时只有客户端B创建的唯一一个临时顺序节点了.\n然后呢，客户端B判断自己居然是集合中的第一个顺序节点，可以加锁了！直接完成加锁，运行后续的业务代码即可，运行完了之后再次释放锁。\n","categories":["zookeeper"]},{"title":"计算机网络读书笔记","url":"/2022/09/03/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/","content":"ARP协议（网络层协议）ARP是解决同一局域网上主机或路由器ip地址和硬件地址映射问题。\n知道机器的ip地址，想要找到机器的硬件地址。\n我们知道网络层使用的是ip地址，但实际网络传送数据帧，使用的还是网络的硬件地址。\n每一台主机上都有一个ARP高速缓存，里面有局域网上的各主机和路由器的ip地址到硬件地址的映射表。\narp请求：源主机向局域网内发送广播，请求内容是（我的ip地址是xxx，硬件地址是xxx，我想知道ip地址是xxx的硬件地址）局域网内所有主机运行的arp进程都会收到这个arp请求。\n若主机B的ip地址和arp请求的目的ip地址相同，就收下这个arp请求分组，并向主机A发送ARP响应分组，同时在响应分组里写入自己的Mac地址。\narp请求完成后，主机A和B的arp高速缓存都会有对方的ip到物理地址的映射。\n使用高速缓存在两台主机频繁的通信效率提高了很多，但是也要给高速缓存设置个过期时间，如果B更换了网络适配器，其他主机高速缓存里放是之前的物理地址，就会找不到B主机。过了过期时间，B的硬件地址已经被删除了，这时其他主机重新发送arp请求分组，找到B主机。\nICMP通常用于返回的错误信息或是分析路由。ICMP错误消息总是包括了源数据并返回给发送者。 ICMP错误消息的例子之一是TTL值过期。每个路由器在转发数据报的时候都会把IP包头中的TTL值减1。如果TTL值为0，“TTL在传输中过期”的消息将会回报给源地址。 每个ICMP消息都是直接封装在一个IP数据包中的，因此，和UDP一样，ICMP是不可靠的。\n传输层局域网中的两台计算机可以通过ip协议 分组里的目的地址进行通信，为什么还需要传输层呢？\n从ip层来说，通信两端是两台主机，实际通信的是主机里的进程。ip协议虽然能把分组发送到目的主机，但分组还是停留在主机的网络层而没有交付主机中的应用进程。从传输层的角度看，端到端的通信就是主机的应用进程之间的通信。\n复用和分用\n复用：发送方不同的应用进程可以使用同一个运输层协议传送数据。\n分用：接收方运输层在剥去报文首部后能够把数据正确交付目的应用进程。\n网络层是为主机之间提供逻辑通信，传输层是为应用进程之前提供端到端的逻辑通信。\nip地址和Mac地址ip地址放在ip数据报的首部，ip地址是网络层及以上用的地址，是逻辑地址（软件实现）\nMac地址是数据链路层和物理层用的地址。\nip数据报一旦交给了数据链路层，就会封装成Mac帧，Mac帧传送时用的源地址和目的地址都是硬件地址，封装在Mac帧的首部。\nip数据报格式总长度\n总长度指首部和数据之和的长度，在IP层下面的每一种链路层协议都规定了一个数据帧中的数据字段最大长度。这是最大传送单元MTU(Maximum transfer unit)，当一个IP数据报封装成链路层的帧时，数据报的总长度（首部加数据部分）不能超过链路层规定的MTU值。最常用的以太网规定其MTU为1500， 如果传送的数据报长度大于MTU，会进行分片处理。\n标识（16位）\n这个字段主要被用来唯一地标识一个报文的所有分片，因为分片不一定按序到达，所以在重组时需要知道分片所属的报文。每产生一个数据报，计数器加1，并赋值给此字段。一些实验性的工作建议将此字段用于其它目的，例如增加报文跟踪信息以协助探测伪造的源地址。\n标志\n这个3位字段用于控制和识别分片，它们是：\n\n位0：保留，必须为0；\n位1：禁止分片（Don’t Fragment，DF），当DF&#x3D;0时才允许分片；\n位2：更多分片（More Fragment，MF），MF&#x3D;1代表后面还有分片，MF&#x3D;0 代表已经是最后一个分片。\n\n如果DF标志被设置为1，但路由要求必须分片报文，此报文会被丢弃。这个标志可被用于发往没有能力组装分片的主机。\n当一个报文被分片，除了最后一片外的所有分片都设置MF为1。最后一个片段具有非零片段偏移字段，将其与未分片数据包区分开，未分片的偏移字段为0。\n分片偏移\n这个13位字段指明了每个分片相对于原始报文开头的偏移量，以8字节作单位。\nTTL字段防止无法交付的数据报一直在因特网中兜圈子（从路由器R1转发到路由器R2，再转发到R3，然后又转发到R1）\n最初的设计是以秒为单位，每经过一个路由器就减去在路由器消耗的一段时间，后来随着科技的进步，路由器处理数据报的时间不断缩短，一般远远小于1秒钟。后来就把TTL字段功能改为跳数限制。路由器在转发数据报之前就把TTL值减1，若TTL值减为0，就丢弃这个数据报，不再转发。 TTL的意义是指明数据报在因特网中最多可经过多少个路由器，TTL占8位，最大就是255。\n若TTL初始值是1，表示这个数据报只能在局域网内传送。因为数据报一传送到某个路由器上，在被转发之前TTL值就减小为0，因而会被这个路由器抛弃。\n首部校验和（16位）\n这个字段只检验数据报首部，不包括数据部分。因为数据报每经过一个路由器，路由器都要重新计算一下首部检验和，不检验数据部分可以减少工作量。通过一种简单的方式检验首部是否发生变化，如果出现差错，丢弃这个数据报。\nMTU和MSS\nMTU由数据链路层提供给网络层的一次传输数据的大小。\nMSS：TCP提交给IP层最大分段大小，假设MTU&#x3D;1500byte，那么MSS&#x3D;1500-20（TCP首部）-20（IP首部）&#x3D;1460byte。\n\n分片和组装\n维基百科-分片和组装\n重组当一个接收者发现IP报文的下列项目之一为真时：\n\nDF标志为0；\n分片偏移量字段不为0。\n\n它便知道这个报文已被分片，并随即将数据、标识符字段、分片偏移量和更多分片标志一起储存起来。\n当接受者收到了更多分片标志未被设置的分片时，它便知道原始数据载荷的总长。一旦它收齐了所有的分片，它便可以将所有片按照正确的顺序（通过分片偏移量）组装起来，并交给上层协议栈。\n应用层协议UDP特点\n\n一对一，一对多，多对一，多对多\n面向报文，UDP的报文不拆分，添加首部后直接传输给ip层。\n没有流量拥塞控制\n头部占用空间小，8字节（tcp首部最少20字节）\n\nTCP怎么保证可靠传输\n停止等待协议\n超时重传，超时计数器\n超时重传时间是略大于一个RTT的时间\n如果时间较小，重发的快，可能包没有丢失就重发，导致网络堵塞\n时间较大，重发的慢，可能丢了半天才重发，影响效率\n确认丢失，确认迟到\n发送方未收到接收方的确认时，不知道是自己发送丢失还是接收方的确认丢失，超时器到期后需要重传分组，接收方又收到了重传的分组，应该立即丢弃，不向上层交付。并且向发送方发送确认。\n累计确认\n接收方不必对每一个分组进行确认，而是在收到几个分组后，对按序到达的最后一个分组发送确认。表示到这个分组为止，所有的分组都收到了。\n累计确认的优点：容易实现，即使确认丢失也不必重传，缺点：不能向发送方反应接收方已经正确收到的所有分组信息。\n流量控制\n粘包问题详细分析TCP粘包\n粘包的原因\n要发送的数据小于TCP发送缓冲区大小，TCP将多次写入缓冲区的数据一次发出去。\n接收端应用层没有及时读取接收缓冲区的数据。\n发送端每次发送后都睡眠一定时间，就不会发生粘包 （等待超时，一般为200ms，第一个包没到MSS长度，但是又迟迟等不到第二个包，则立即发送）\n怎么处理粘包\n粘包的原因是不确定消息的边界，只要在发送端每次发送消息的时候给消息带上识别边界的信息，接收端就可以根据消息的边界，区分出每个消息。\n加入特殊标志\n通过特殊的标志作为头尾，比如收到0xfffffe或者回车，认为收到消息头，直到收到下一个头标志或者尾部标记，才认为是一个完整消息\n加入消息长度消息\n收到头标志时，里面还可以带上消息长度，表名之后有多少byte都是属于这个消息。\n因为TCP是基于流的，发的时候就不保证发的是一个完整的数据报，这串字节流在接收端收到时哪怕知道长度也没用，因为它很可能只是某个完整消息的一部分。\nIP层不会有粘包问题，\n消息过长的话IP层会按照MTU的长度把消息分成N个切片，每个切片带有自身在包里的offset和同样的IP头信息。\n","categories":["计算机网络原理"]},{"title":"面试总结","url":"/2022/03/12/%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/","content":"\n重写和重载\n\n重写是 子类继承父类, 然后子类可以重写父类的方法\n重写时,参数类型,参数个数,方法名都要和被重写的方法完全一致!\n重写时,父类private方法不能被重写(爸爸的隐私,孩子不能看)\n子类重写的方法不能比父类的修饰符更严格\n重载发生在一个类中,方法名相同,参数个数,参数类型,参数顺序 不相同 \n\nRedis持久化方式 -RDB和AOF的优缺点\n\nRDB是快照方式需要恢复数据的时候直接把RDB快照加载就行\n如果数据较多, 保存快照的时间比较久\nAOF是写命令追加到日志, 日志文件会越来越大\n\nRDB的缺点\n\nRDB快照一般都是每五分钟或者更长时间更新一次,如果Redis宕机 可能会丢失近几分钟的数据. 这个问题就是RDB最大的缺点, 不适合做第一恢复方案\nRDB fork子进程进行数据备份,如果数据量特别大,等待时间变久,.\n所以 一般不要让RDB备份时间间隔太久, 否则每次生成的RDB文件太大了，对Redis本身的性能可能会有影响的；\n\nAOF优缺点\n\n一般AOF会每隔1秒，通过一个后台线程执行一次fsync操作，最多丢失1秒钟的数据，Redis进程挂了，最多丢掉1秒钟的数据；\nAOF日志文件以append-only模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修；\n\nMySQL主键索引有什么用\n\n主键保证唯一性 &amp;&amp; 主键列不为NULL\n\nMySQL表主键是必须的吗\n\n主键不是必须的,简单的表可以不设置主键, 设置主键能提高表的查询效率\n如果没有定义主键,innoDB会选择唯一非空索引作为主键,如果没有的话 innoDB会自动生成一个包含了ROW_ID值的列作为聚簇索引,行都会根据这个ROW_ID排序。\n\ncallable接口的返回值用什么接收\n\nCallable相比Runnable接口的好处\nCallable接口 有返回值,返回值用FutureTask的get方法接收Callable接口可以抛出异常\nclass CallTest implements Callable&#123;\t@Override\tpublic Integer call()&#123;\t\treturn 123;\t&#125;&#125;public static void main(String[] args) throws Exception &#123;//\t\tnew Thread(new Runable1(), &quot;RunnableName&quot;).start();\t\tFutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;&gt;(new CallTest());\t\t//lambda表达式优化\t\tFutureTask&lt;String&gt; futureTask1=new FutureTask&lt;&gt;(()-&gt; &quot;lambda表达式优化&quot;);\t\t\t\tnew Thread(futureTask1).start();\t\tSystem.out.println(&quot;Callable接口(lambda表达式)返回数据&quot;+futureTask1.get());\t\tnew Thread(futureTask).start();\t\tSystem.out.println(&quot;Callable接口返回数据&quot;+futureTask.get());\t&#125;&#125;\n\n\n\n\nsleep() 方法 和wait() 方法\n\nsleep方法是Thread类下的\n线程调用sleep方法虽然休眠了 但是不会释放锁对象\nwait() 方法是Object类下的\nwait方法会让出CPU的执行权, 调用wait方法后 需要用notify&#x2F;notifyall 来唤醒线程\n/** * @ Author     ：赵棱泉. * @ Date       ：Created in 23:21 2022/2/1 * @ Description： */public class Task implements Runnable &#123;\tprivate static synchronized void threadtest() &#123;\t\tSystem.out.println(Thread.currentThread().getName() + &quot;获取到了锁&quot;);\t\ttry &#123;\t\t\tThread.sleep(2000);\t\t&#125; catch (InterruptedException e) &#123;\t\t\te.printStackTrace();\t\t&#125;\t\tSystem.out.println(Thread.currentThread().getName() + &quot;释放锁&quot;);\t&#125;\tpublic static void main(String[] args) &#123;\t\tnew Thread(new Task()).start();\t\tnew Thread(new Task()).start();\t&#125;\t@Override\tpublic void run() &#123;\t\tthreadtest();\t&#125;&#125;\n\n\n\n子类可以重写父类的构造方法吗\n\n不能!!\n每个类的构造方法和类名都是相同的,\n如果重写了构造方法 相当于子类和父类名是相同的, 不可实现\n\nMySQL排序关键字 和分组关键字\n\n排序关键字order by\n分组关键字group by\n\nRedis 缓存雪崩讲一下\n\n缓存雪崩指的是大面积key值在同一时间段失效,大量的请求落到数据库上,导致数据库宕机\n","categories":["面试总结"],"tags":["interview"]}]